---
title: "STAT 542 Homework # 1 "
author: "Wei Zhong (NetID: wzhong8)"
date: "Sep 17th, 2017"
header-includes:
   - \usepackage{bbm}
   - \usepackage{dsfont}
   - \usepackage{physics}
   - \usepackage{amsmath}
   - \usepackage{bm}
output:
  pdf_document:
    fig_width: 7
    fig_height: 3.2
    fig_caption: true
geometry: margin = 1in
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1
### a) Calculate the sample variance-covariance matrix $\widehat{\Sigma}$ of $X$ (using the maximum likelihood estimator, not the biased version). Then calculate $\widehat{\Sigma}^{-1/2}$.

```{r question1a}
rm(list = ls())

library(MASS)
set.seed(1)
P = 4
N = 200
rho = 0.5
V = rho^abs(outer(1:P, 1:P, "-"))
X = as.matrix(mvrnorm(N, mu=rep(0,P), Sigma=V))
beta = as.matrix(c(1, 1, 0.5, 0.5))
Y = X %*% beta + rnorm(N)

# sample variance-covariance matrix of X
k = ncol(X)
n = nrow(X)

# creates means for each column 
X_means = rep(1, n) %*% t(as.matrix(apply(X, 2, mean)))

# creates a difference matrix
D = X - X_means
test = apply(X, 1, mean)
# creates the covariance matrix
C = (1/(n)) * (t(D) %*% D)
C

decsig = eigen(C)
decsig$vectors %*% (decsig$values^(-1/2) * t(decsig$vectors))

```

### b) Perform a 5-NN estimation at the target point $x = (0.5, 0.5, 0.5, 0.5)^T$. 
The 5-NN estimation at the target point is 1.346639.
```{r question1b}
# target point
xtar = rep(0.5, 4)

# Euclidean distance between any two vectors 
mydist = function(x1, x2){
  dist = sqrt( sum((x1- x2)^2) )
  return(dist)
}

distar = rep(0, nrow(X))

for (i in 1: nrow(X)){
  distar[i] = mydist(X[i, ], xtar)
}

# the row numbers of the closest 5 subjects.
smallest5 = which(distar <=  sort(distar)[5])

# their Y values
mean(Y[smallest5])


```

### c) Find the 5-NN estimation based on this Mahalanobis distance with $s= \widehat{\Sigma}$.
The 5-NN estimation at the target point is 0.5615577.
```{r question1c}

# Mahalanobis distance function
mydist2 = function(x1, x2, s){
 d = x1-x2
 mahal = t(d) %*% solve(s) %*% d
 return(mahal)
}

distar2 = rep(0, nrow(X))

for (i in 1: nrow(X)){
  distar2[i] = mydist2(X[i, ], xtar, s = C)
}

# the row numbers of the closest 5 subjects.
smallest5_ma = which(distar2 <=  sort(distar2)[5])

# their Y values
mean(Y[smallest5_ma])


```

### d) Which estimator seems perform better? Given any explanation.

The MSE of Euclidean 5-NN estimation is smaller than Mahalanobis distance way. Thus the estimator using Euclidean distance seems better.
```{r question1d}

realy = xtar %*% beta
(mean(Y[smallest5])- realy)^2
(mean(Y[smallest5_ma])- realy)^2

```

## Quetion 2 
### a) If we are interested in using $k = 5$, derive the degrees of freedom of this model using the given formula.

A linear smooth has the form:
$$\hat{r} (x) = \sum_{j=1}^n w(x, x_j) \cdot y_j$$
This means that 
$$\hat{y}_i = \hat{r}(x_i) = \sum_{j=1}^n w(x_i, x_j) \cdot y_j$$
Let $\hat{y} = Sy$, for the matrix $S\in R^{n\times n}$ defined as $S_{ij} = w(x_i, x_j)$. Calculating the degree of freedom, in matrix form:
$$ \begin{aligned}
d(\hat{y}) &= \frac{1}{\sigma^2}\text{tr}(\text{Cov}(Sy, y))\\
& =\frac{1}{\sigma^2}\text{tr}(S\text{Cov}(y, y)) \\
& = tr(S)\\
&= \sum_{i=1}^n w(x_i, x_j)
\end{aligned}$$

Consider k-nearest-neighbors regression with some fixed value of $k \geq 1$. Recall that here 

$$
w(x, x_j)  = \left\{
        \begin{array}{ll}
            \frac{1}{k} & \quad x_j \text{is one of the k closest points to x}  \\
            0 & \quad \text{else}
        \end{array}
    \right.
$$
Therefore $w(x_i, x_j) = 1/k$, and 
$$d(\hat{y}^{knn}) = \sum_{i=1}^n \frac{1}{k} = \frac{n}{k}$$
Thus the degree of freedom in this case is $200/5 = 40$.



```{r question2a}
k = 5
df = N/k
```

### b) Perform the simulation study.
The estimated degree of freedom is about 41.2429, compared to theoretical degree of freedom 40.

```{r question2b}
# Generate a design matrix X from independent standard normal distribution 
# with n = 200 and p = 4.

set.seed(22)
desx = matrix(rnorm(N * P, mean = 0, sd = 1), N, P) 

library(kknn)

fittedy = matrix(0, nrow = N, ncol = 20)
truey = matrix(0, nrow = N, ncol = 20)
empvar = rep(0, 200)

for (i in 1: 20){
  truey1 = desx %*% beta + rnorm(N)
  
  knn.fit = kknn(truey1 ~ desx, train = data.frame(x = desx, y = truey1), 
               test = data.frame(x = desx, y = truey1),
               k = 5, kernel = "rectangular")
  fittedy[, i] = knn.fit$fitted.values
  truey[, i] = as.vector(truey1)
}



for(i in 1: N){
   empvar[i] = var(truey[i, ], fittedy[i, ])
}

sum(empvar)
```


### c) Consider a linear model $\bm{y = X\beta + \epsilon}$, and the fitted value from linear regression $\bm{\hat{y} = X\hat{\beta} = X(X^TX)^{-1}X^Ty}$. Assume that $\epsilon$'s are iid normal with mean 0 and variance $\sigma^2$. What is the theoretical degrees of freedom for this linear regression?

Recall that 
$$\hat{y} = X\hat{\beta} = X(X^TX)^{-1}X^Ty$$
Here $X$ is the $n\times p$ predictor matrix. Then replying on the matrix form of degrees of freedom, 
$$\begin{aligned}
\text{df}(\hat{y}) & = \frac{1}{\sigma^2}\text{tr}(\text{Cov}( X(X^TX)^{-1}X^Ty, y)) \\
& =  \frac{1}{\sigma^2} \text{tr} (X(X^TX)^{-1}X^T \text{Cov}(y, y)) \\
& = \text{tr}(X(X^TX)^{-1}X^T) \\
& = \text{tr}(X^T X(X^T X)^{-1}) \\
& = p
\end{aligned}$$
So the theoretical degrees of freedom for this linear regression is $p$.


## Question 3
The best $k$ for this 10 crossvalidation is 44. The red line in the plot is test error and green line is train error. 
```{r}
set.seed(1)
library(ElemStatLearn)
dim(SAheart)
# 10 fold cross validation
library(class)
nfold = 10
infold = sample(rep(1:nfold, length.out = dim(SAheart)[1]))

mydata = data.frame(x = cbind(SAheart$age, SAheart$tobacco), y = SAheart$chd)

K = 50 # maximum number of k 
errorMatrix = matrix(NA, K, nfold) # save the prediction error of each fold
trainerrorMatrix = matrix(NA, K, nfold) # save the train error of each fold

for (l in 1:nfold)
{
	for (k in 1:K)
	{knn.predict = knn(train = mydata[infold != l, -3], 
	                    test = mydata[infold == l, -3], 
	                    cl = mydata[infold != l, 3], k=k)
	 
		errorMatrix[k, l] = 1- mean((as.numeric(knn.predict)-1) == mydata$y[infold == l])
		
		knn.predicttrain = knn(train = mydata[infold != l, -3], 
	                    test = mydata[infold != l, -3], 
	                    cl = mydata[infold != l, 3], k=k)
		
		trainerrorMatrix[k, l] = 1- 
		  mean((as.numeric(knn.predicttrain)-1) == mydata$y[infold != l])
	}
}

# plot the results
plot(rep(1:K, nfold), as.vector(errorMatrix), pch = 19, cex = 0.5)
points(1:K, apply(errorMatrix, 1, mean), col = "red", pch = 19, type = "l", lwd = 3)
points(1:K, apply(trainerrorMatrix, 1, mean), col = "green", pch = 19, type = "l", lwd = 3)

# which k is the best?
which.min(apply(errorMatrix, 1, mean))





```